---
title: Introduction
description: "Arroyo Stream Processing Engine"
---


[Arroyo](https://arroyo.dev) is a distributed stream processing engine written in Rust, designed to efficiently
perform stateful computations on streams of data. Unlike traditional batch processing, streaming engines can operate
on both bounded and unbounded sources, emitting results as soon as they are available.

In short: Arroyo lets you ask complex questions of high-volume real-time data with subsecond results.

## Features

* SQL and Rust pipelines
* Scales up to millions of events per second
* Stateful operations like windows and joins
* State checkpointing for fault-tolerance and recovery of pipelines
* Timely stream processing via the [Dataflow model](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/)

## Use cases

Some example use cases include:

* Detecting fraud and security incidents
* Real-time product and business analytics
* Real-time ingestion into your data warehouse or data lake
* Real-time ML feature generation

## Why Arroyo

There are already a number of existing streaming engines out there, including [Apache Flink](https://flink.apache.org),
[Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html), and
[Kafka Streams](https://kafka.apache.org/documentation/streams/). Why create a new one?

* **Serverless operations**: Arroyo pipelines are designed to run in modern cloud environments, supporting seamless scaling,
    recovery, and rescheduling
* **High performance SQL**: SQL is a first-class concern, with consistently excellent performance
* **Designed for non-experts**: Arroyo cleanly separates the pipeline APIs from its internal implementation. You don't
    need to be a streaming expert to build real-time data pipelines.

## Current state

Arroyo is currently alpha. It is missing featurs and has known bugs. At this stage, it is likely to primarily be of
interest to the Rust data processing community and contributors.

## Getting Started

### With Docker

The easiest way to get started running Arroyo is with the single-node Docker image. This contains all of the services
you need to get a test Arroyo system up and running.

```bash
$ docker run -p 8000:8000 -p 8001:8001 ghcr.io/arroyosystems/arroyo:single
```

This will run all of the core arroyo services and a temporary Postgres database. The web UI will be exposed at
http://localhost:8000.

Note that this mode should not be used for production, as it does not persist configuration state and does not support
any distributed runtimes.

### Locally

Arroyo can easily be run locally as well. Currently Linux and MacOS are well supported as development environments.

To run Arroyo locally first follow the dev setup instructions to get a working dev environment. Then, you can start
the API and Controller services:

```bash
$ target/release/arroyo-controller
# in another terminal
$ target/release/arroyo-api
```

Then you can navigate to the web ui at http://localhost:8000.

### Next steps

Now you're ready to start building your first Arroyo pipeline! Check out the [tutorial](/tutorial) to get started.
