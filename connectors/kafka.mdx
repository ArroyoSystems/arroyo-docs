---
title: Kafka
description: "Use Kafka topics as sources and sinks"
---

Arroyo provides sources and sinks for [Apache Kafka](https://kafka.apache.org/)
for consistently reading and writing data from Kafka topics using exactly-once
semantics. Kafka's distributed log model makes it a great fit for stream
processing applications like Arroyo.

Arroyo supports Kafka clusters that are self-hosted, using Amazon
MSK<sup>1</sup>, and Confluent Cloud.

*<sup>1</sup> Note that MSK Serverless is not currently supported due to its
reliance on non-standard IAM authentication*

## Configuring the Connection

Kafka connections can be created via the Web UI or via the API.

<img
src="/images/create_kafka.png"
alt="Kafka creation flow"
style={{ height: "600px" }}
/>

A Kafka connection has several required and optional fields:

| Field | Description | Required | Example |
| ----- | ----------- | -------- | ------- |
| bootstrap_servers | A comma-separated list of Kafka servers to connect to | Yes | `kafka-server-1.cluster:9092,kafka-server-2.cluster:9092` |
| auth.type | One of 'none' or 'sasl' | No | `sasl` |
| auth.protocol | The SASL protocol to use (e.g., `SASL_PLAINTEXT`, `SASL_SSL`) | No | `SASL_PLAINTEXT` |
| auth.mechanism | The SASL mechanism to use (e.g., `SCRAM-SHA-256`, `SCRAM-SHA-512`) | No | `SCRAM-SHA-256` |
| auth.username | The username to use for SASL authentication | No | `user` |
| auth.password | The password to use for SASL authentication | No | `password` |
| topic | The name of the Kafka topic to read from | Yes |
| type | The type of table (either 'source' or 'sink') | Yes | `source` |
| source.offset | The offset to start reading from (either 'earliest' or 'latest') | No | `earliest` |


## Kafka Sources

Kafka sources can be created via the Web UI, the API, or directly in SQL. A
Kafka source is defined by a topic name and a schema.

Schemas can be defined via json-schema, or automatically configured via
Confluent Schema Registry.

Kakfa sources implement exactly-once semantics by storing the last-read offset
in Arroyo's state.

## Kafka Sinks

Kafka sinks can be created via the Web UI, the API, or directly in SQL. A Kafka
sink is defined by a topic name. Currently, Kafka sinks only support writing
JSON data, with the structure determined by the schema of the data being
written.

The Kafka sink does not yet support transactional semantics, so double-writes
are possible if a pipeline fails and must recover from earlier in time
(at-least-once semantics). Exactly-once support for Kafka sinks is planned.

## Kafka DDL

Kafka connector tables can defined via SQL DDL and used as sources and sinks in
SQL queries. In the `CREATE TABLE` statement, the following options are
supported:

| Option | Description | Required |
| ------ | ----------- | -------- |
| connection | The name of the Kafka connection to use | Yes |

For Example:

```sql
CREATE TABLE orders (
  customer_id INT,
  order_id INT
) WITH (
  connector = 'kafka',
  format = 'json',
  bootstrap_servers = 'kafka-server-1.cluster:9092,kafka-server-2.cluster:9092',
  topic = 'order_topic',
  type = 'source',
  'source.offset' = 'earliest'
);
```
