---
title: Overview
description: Connect Arroyo to external systems
---

Arroyo interacts with other data systems via _connectors_, which can implement sources and sinks for reading and writing
data respectively. The list of connectors is constantly expanding. If you'd like to connect Arroyo with a system that's
not currently supported, please get in touch with the team on Discord or via GitHub issues.

## Supported Connectors

| Name | Source | Sink | Availability |
| ---- | ------ | ---- | ----|
| [Kafka](/connectors/kafka) | Yes | Yes | OSS |
| [Redpanda](/connectors/redpanda) | Yes | Yes | OSS |
| [Server-Sent Events](/connectors/server-sent-events) | Yes | No | OSS|
| [MySQL](/connectors/mysql) | Yes | Yes | OSS |
| [Postgres](/connectors/postgres) | Yes | Yes | OSS |
| [State Sink](/connectors/state) | No | Yes | Arroyo Cloud |

<img src="/images/create_source.png" alt="Create source dialog" />

## Using connectors

Connecting with an external system happens in two steps. First, you create the _connection_, which describes how
to talk with the external system (generally this will involve configuring an endpoint and setting up authentication if
required). This connection can be shared by all users of that external system; so for example, there would be a single
connection for a particular Kafka cluster.

Connections can be created either in the Web UI or via the API. Once a connection has been created, it can be used to
create a _source_ or _sink_. A source is used to read data from an external system, and a sink is used to write data to
an external system.

Sources and sinks can also be created in the Web UI or API, which allows them to be reusable across multiple pipelines.
A typical pattern is that the data engineering team may set up, for example, a source for a particular Kafka topic,
along with its schema, which can then be used by pipeline authors.

Sources and sinks can also be created directly in SQL by using DDL statements, as described in the
[SQL docs](/sql/ddl).

See the individual connector docs for details on how to configure them in Arroyo.

## Source schemas

Sources in Arroyo must have associated schemas to allow them to be used in SQL queries. Schemas describe how to
interpret the data, mapping it into a table composed of [Arroyo SQL types](/sql/data-types).

Schemas can be defined when creating the source in the web UI or API, which allows them to be reused across queries.
Arroyo supports several methods of schema definition, some of which are also associated with a particular data format:

* [Json Schema](https://json-schema.org/)
* [Avro](https://avro.apache.org/docs/current/spec.html) (in progress)
* [Protobuf](https://developers.google.com/protocol-buffers) (in progress)

JSON sources may also be configured as `Raw Json`, which means that the data will be available in SQL as a table with
a single column called `value`, with type `VARCHAR`. This can then be parsed dynamically with the
[SQL JSON functions](/sql/scalar-functions#json-functions).

Schemas can also be read automatically from
[Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html).
