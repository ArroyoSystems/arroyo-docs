---
title: Overview
description: Connect Arroyo to external systems
---

Arroyo interacts with other data systems via _connectors_, which can implement sources and sinks for reading and writing
data respectively. The list of connectors is constantly expanding. If you'd like to connect Arroyo with a system that's
not currently supported, please get in touch with the team on Discord or via GitHub issues.

## Supported Connectors

| Name | Source | Sink | Availability |
| ---- | ------ | ---- | ----|
| [Blackhole](/connectors/blackhole) | No | Yes | OSS |
| [Filesystem](/connectors/filesystem) | No | Yes | OSS |
| [Fluvio](/connectors/fluvio) | Yes | Yes | OSS |
| [Impulse](/connectors/impulse) | Yes | No | OSS |
| [Kafka](/connectors/kafka) | Yes | Yes | OSS |
| [Kinesis](/connectors/kinesis) | Yes | Yes | OSS |
| [MySQL](/connectors/mysql) | Yes | Yes | OSS |
| **Nexmark** | Yes | No | OSS |
| [Polling HTTP](/connectors/polling-http) | Yes | No | OSS |
| [Postgres](/connectors/postgres) | Yes | Yes | OSS |
| [Redpanda](/connectors/redpanda) | Yes | Yes | OSS |
| [Server-Sent Events](/connectors/server-sent-events) | Yes | No | OSS|
| [State](/connectors/state) | No | Yes | Arroyo Cloud |
| [Webhook](/connectors/webhook) | No | Yes | OSS |
| [WebSocket](/connectors/websocket) | Yes | No | OSS |

<img src="/images/create_connection.png" alt="Create connection dialog" />

## Using connectors

Arroyo SQL supports a special kind of table called a _connection table_ which is
used to interact with external systems as sources or sinks.

Connection tables come in two forms. _Saved connections_ are created in the Web UI
or via the API and can be easily reused across queries. Existing saved connections
can be viewed via the Connections tab of the Web UI, and their schemas can be seen
in the catalogue view in the SQL editor.

Connection tables can also be created via `CREATE TABLE` statements in SQL, as described
in the [DDL](/sql/ddl) docs.

A particular connection table is either a _source_ (meaning it reads data from
an external system) or a _sink_ (meaning it writes data). Some connectors support
only one of these, while others support both.

See the individual connector docs for details on on their capabilities and how
to configure them in Arroyo.

## Connection formats

Some connectors support arbitrary data formats, while others have a fixed format defined
by the connector. For example, Kafka stores data as a sequence of bytes, which can be
interpreted in many different ways. When creating a connection it's necessary to specify
the format of the data so that Arroyo can serialize or deserialize it.

Arroyo currently supports the following formats:

* `json`
* `debezium_json`
* `raw_string`
* `parquet`

Formats may be specified via the `format` option the `CREATE TABLE` `WITH` clause,
or via the web ui. Some formats have additional options that can be set. See their
individual docs for details.

### Json

JSON is a common format for data interchange. Arroyo supports JSON in two forms:

* `json` - arbitrary JSON data
* `debezium_json` - JSON data in the format produced by [Debezium](https://debezium.io/)

The following options are supported for both formats:

| Option | Description | Default |
| ------ | ----------- | ------- |
| `json.confluent_schema_registry` | Set to true if data was produced by a [Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html) connected source | `false` |
| `json.include_schema` | Set to true to include the schema in the output, allowing it to be used with Kafka Connect connectors that require a schema | `false` |
| `json.unstructured` | Set to true to treat the data as unstructured JSON, which will be parsed as a single column of type `TEXT` | `false` |
| `json.timestamp_format` | The format of timestamps in the data. May be one of `rfc3339` or `unix_millis` | `rfc3339` |


### Raw string

Raw string data is treated as a single column named `value` with type `TEXT`,
and can be operated on using
[SQL string functions](/sql/scalar-functions#string-functions) or UDFs.

Raw string is supported for both deserialization (from sources) and serialization
(to sinks). As a serialization format, it can be useful for generating data in formats
that Arroyo does not support natively, for example via UDFs:

```rust
fn my_to_json(f: f64) -> String {
    let v = serde_json::json!({
        "my_complex": {
            "nested_format": f
        }
    });

    serde_json::to_string(&v).unwrap()
}
```

### Parquet

[Parquet](https://parquet.apache.org/) is a columnar data format that is commonly
used for storing data in data lakes. Arroyo supports writing Parquet via the FileSystem
sink. Refer for the [FileSystem sink docs](/connectors/filesystem) for details.

## Framing

The framing configuration for a source determines how messages read from the source
are split into records for processing. By default, this is set to `none`, which
means that the source will emit a single record for each message it reads. This is
particularly useful for sources—like HTTP endpoints—that do not have their own
framing protocol.

Framing is configured via the `framing` option when creating a SQL table. Currently
only `newline` is supported, which splits the input on newlines.

You may also set `framing.newline.max_length` to a number of bytes to limit the
maximum length of a single record. Records that exceed this length will be truncated.

For example:

```sql
CREATE TABLE my_source (
    value TEXT
) WITH (
    ...
    framing = 'newline',
    'framing.newline.max_length' = '10000'
)
```

## Connection schemas

Connections in Arroyo must have associated schemas to allow them to be used in
SQL queries. Schemas describe how to interpret the data, mapping it into a table
composed of [Arroyo SQL types](/sql/data-types).

Schemas can be defined when creating the source in the web UI or API, which
allows them to be reused across queries. Arroyo supports several methods of
schema definition, some of which are also associated with a particular data
format:

* [Json Schema](https://json-schema.org/)
* [Avro](https://avro.apache.org/docs/current/spec.html) (in progress)
* [Protobuf](https://developers.google.com/protocol-buffers) (in progress)

JSON sources may also be configured as `Raw Json`, which means that the data
will be available in SQL as a table with a single column called `value`, with
type `TEXT`. This can then be parsed dynamically with the
[SQL JSON functions](/sql/scalar-functions#json-functions).

Schemas for Kafka topics can also be read automatically from
[Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html).


## Source idleness

Paritioned sources (like Kafka or Kinesis) may experience periods when some
partitions are active but others are idle due to the way that they are keyed.
This can cause issues in Arroyo due to how [watermarks](/concepts/#watermarks) are
calculated: as the minimum of the watermarks of all partitions.

If some partitions are idle, the watermark will not advance, and queries that
depend on it will not make progress. To address this, sources support a concept
of _idleness_, which allows them to mark partitions as idle after a period of
inactivity. Idle partitions, meanwhile, are ignored for the purpose of calculating
watermarks and so allow queries to advance.

Idleness is enabled by default for all sources with a period of 5 minutes. It can be
configured when creating a source in SQL by setting the `idle_micros` options, or disabled
by setting it to `-1`.

A special case of idleness occurs when there are more Arroyo source tasks than partitions
(for example, a Kafka topic with 4 partitions read by 8 Arroyo tasks). This means that
some tasks will never receive data, and so will never advance their watermarks. This can
occur as well for non-partitioned sources like WebSocket, where only a single task is
able to read data.

To address this, source tasks with no work assigned to them will mark themselves as idle
immediately.
