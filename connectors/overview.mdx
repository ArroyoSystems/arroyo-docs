---
title: Overview
description: Connect Arroyo to external systems
---

Arroyo interacts with other data systems via _connectors_, which can implement sources and sinks for reading and writing
data respectively. The list of connectors is constantly expanding. If you'd like to connect Arroyo with a system that's
not currently supported, please get in touch with the team on Discord or via GitHub issues.

## Supported Connectors

| Name | Source | Sink | Availability |
| ---- | ------ | ---- | ----|
| [Blackhole](/connectors/blackhole) | No | Yes | OSS |
| [Filesystem](/connectors/filesystem) | Yes | Yes | OSS |
| [Fluvio](/connectors/fluvio) | Yes | Yes | OSS |
| [Impulse](/connectors/impulse) | Yes | No | OSS |
| [Kafka](/connectors/kafka) | Yes | Yes | OSS |
| [MySQL](/connectors/mysql) | Yes | Yes | OSS |
| **Nexmark** | Yes | No | OSS |
| [Postgres](/connectors/postgres) | Yes | Yes | OSS |
| [Redpanda](/connectors/redpanda) | Yes | Yes | OSS |
| [Server-Sent Events](/connectors/server-sent-events) | Yes | No | OSS|
| [State](/connectors/state) | No | Yes | Arroyo Cloud |
| [WebSocket](/connectors/websocket) | Yes | No | OSS |

<img src="/images/create_connection.png" alt="Create connection dialog" />

## Using connectors

Arroyo SQL supports a special kind of table called a _connection table_ which is
used to interact with external systems as sources or sinks.

Connection tables come in two forms. _Saved connections_ are created in the Web UI
or via the API and can be easily reused across queries. Existing saved connections
can be viewed via the Connections tab of the Web UI, and their schemas can be seen
in the catalogue view in the SQL editor.

Connection tables can also be created via `CREATE TABLE` statements in SQL, as described
in the [DDL](/sql/ddl) docs.

A particular connection table is either a _source_ (meaning it reads data from
an external system) or a _sink_ (meaning it writes data). Some connectors support
only one of these, while others support both.

See the individual connector docs for details on on their capabilities and how
to configure them in Arroyo.

## Connection formats

Some connectors support arbitrary data formats, while others have a fixed format defined
by the connector. For example, Kafka stores data as a sequence of bytes, which can be
interpreted in many different ways. When creating a connection, it's necessary to specify
the format of the data so that Arroyo can deserialize or serialize it.

Arroyo supports the following formats:

* json
* avro (in progress)
* protobuf (in progress)
* raw_string
* parquet

Formats may be specified via the `format` option the `CREATE TABLE` `WITH` clause,
or via the web ui.

Some formats have additional options that can be configued via `format_options`:

* `format_options.json_schema_registry`: configures the connection to use
  [the Confluent Schema Registry wire format](https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#wire-format).


## Connection schemas

Connections in Arroyo must have associated schemas to allow them to be used in
SQL queries. Schemas describe how to interpret the data, mapping it into a table
composed of [Arroyo SQL types](/sql/data-types).

Schemas can be defined when creating the source in the web UI or API, which
allows them to be reused across queries. Arroyo supports several methods of
schema definition, some of which are also associated with a particular data
format:

* [Json Schema](https://json-schema.org/)
* [Avro](https://avro.apache.org/docs/current/spec.html) (in progress)
* [Protobuf](https://developers.google.com/protocol-buffers) (in progress)

JSON sources may also be configured as `Raw Json`, which means that the data
will be available in SQL as a table with a single column called `value`, with
type `TEXT`. This can then be parsed dynamically with the
[SQL JSON functions](/sql/scalar-functions#json-functions).

Schemas for Kafka topics can also be read automatically from
[Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html).
