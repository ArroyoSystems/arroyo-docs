When using field-based partitioning and high parallelism, you may end up with many files;
typically each sink subtask will write a file for every partition key. To avoid this,
you can configure the dataflow to insert a shuffle step before the sink, which will ensure
that all records for a particular partition key end up on the same sink node:

```
'shuffle_by_partition.enabled' = true
```

For example, if our partition key is `event_type` and we have 100 distinct
types, at parallelism 32 we'd end up with 3,200 files being written for each
flush interval. By enabling shuffle_by_partition, we reduce that 100.

Note that this may lead to performance problems if your data is highly skewed
across your partition keys; for example, if 90% of your data is in the same
partition, those events will all end up on the same sink subtask which may
not be able to keep up with the volume.
